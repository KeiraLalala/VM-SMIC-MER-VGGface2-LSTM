{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wednesday Oct 14 08:51 2020\n",
    "Used to pre-process dataset for Phase-based video magnification\n",
    "@author: Keira - github.com/Keira. Bai\n",
    "a.function to divide data set into 5 groups for cross-validation, return a 5*N dimention matrix \n",
    "b.function to extract trainiing data into fixed size sequences, sequences consist by consecuous internal\n",
    "c.function to extract validation data, sequences consist by raw image and adding frames to the same size\n",
    "d.function to read image\n",
    "e.function to magnify frames in slide window\n",
    "\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import sys\n",
    "import torch\n",
    "import glob as gb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from perceptual.filterbank import *\n",
    "from pyr2arr import Pyramid2arr\n",
    "from torchvision import transforms\n",
    "from temporal_filters import IdealFilterWindowed, ButterBandpassFilter\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocate dataset into k groups for training and validation\n",
    "def CrossAllocation(datapaths,k):\n",
    "    neg_dir = \"*/negative/*\"\n",
    "    pos_dir = \"*/positive/*\"\n",
    "    sur_dir = \"*/surprise/*\"\n",
    "    non_dir = \"non_micro/*\"\n",
    "    imgpath = \"/*.bmp\"\n",
    "    sequ_list = []\n",
    "    label_list = []\n",
    "    Val_tra_tes_list = []\n",
    "    label = 0\n",
    "    #Load in all folders adding expression\n",
    "    for vid_dir in [neg_dir, pos_dir, sur_dir, non_dir]:        \n",
    "        sequ = gb.glob(datapaths+vid_dir)#read all folders under different expression\n",
    "        sequ_list += sequ \n",
    "        label_list += [label for i in range(len(sequ))] #read in the same number of expression label\n",
    "        label += 1 #change the expression category\n",
    "    #Devide All Data into k groups\n",
    "    for i in range(k):\n",
    "        if k-i>1:\n",
    "            train_list, valid_list, train_label, valid_label = train_test_split(sequ_list, label_list, \\\n",
    "                                          test_size=1/(k-i), random_state=42)\n",
    "            Val_tra_tes_list.append([valid_list, valid_label])\n",
    "            \n",
    "        else:\n",
    "            Val_tra_tes_list.append([train_list, train_label])\n",
    "\n",
    "        sequ_list = train_list\n",
    "        label_list = train_label   \n",
    "\n",
    "    return Val_tra_tes_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation on sequence level by fixed size\n",
    "def InputImagewithSlide(tra_tes_list, seleframe = 11):    \n",
    "    imgpath = \"/*.bmp\"\n",
    "    sequ_list = []\n",
    "    \n",
    "    exp_list = []\n",
    "    step_list = [1,2,3,4,5,6]\n",
    "    expCount = [0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for group in tra_tes_list:    \n",
    "        for folder, exp in zip(group[0], group[1]):\n",
    "            img_list = sorted(gb.glob(folder + imgpath))#get frames from the same one folder\n",
    "            img_len = len(img_list)\n",
    "            expCount[exp] += img_len#recording the same number of expression \n",
    "            padSeq = []\n",
    "            #repackage training sequence            \n",
    "            if img_len < seleframe: #for frames less than seleframe\n",
    "                padSeq.extend(img_list)\n",
    "                padSeq.extend(img_list[:seleframe-img_len])\n",
    "                sequ_list.append(padSeq)#append the sequence directly\n",
    "                exp_list.append(exp)\n",
    "                expCount[exp+4] += seleframe\n",
    "            else: #for frames more than seleframe\n",
    "                #control step to keep the amout balance between expressions\n",
    "                if exp == 3:#non-micro-expression\n",
    "                    step = 3#decrease the growth rate\n",
    "                else:\n",
    "                    step = 1\n",
    "                for i in range(0, img_len-seleframe+step, step):\n",
    "                        if ((img_len-i) >= (seleframe+step-1)):#sequence is enough for a seleframe+step\n",
    "                            for s in step_list:#assign inner step for frame\n",
    "                                if img_len-i >= seleframe*s: #if frame number is enough for the inner step\n",
    "                                    sequence = []\n",
    "                                    for j in range(0,seleframe*s,s):\n",
    "                                        sequence.append(img_list[j+i]) #frames for a window size  \n",
    "                                    sequ_list.append(sequence)#adding sequence for sequence list\n",
    "                                    exp_list.append(exp)\n",
    "                                    expCount[exp+4] += seleframe\n",
    "                        else:\n",
    "                            if step != 1:\n",
    "                                m = img_len-seleframe\n",
    "                                sequence = []\n",
    "                                for j in range(seleframe):\n",
    "                                    sequence.append(img_list[j+m])#adding the frames in the end part of video \n",
    "                                sequ_list.append(sequence)\n",
    "                                exp_list.append(exp)\n",
    "                                expCount[exp+4] += seleframe\n",
    "\n",
    "    return sequ_list, exp_list, expCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data for validation\n",
    "def RawforVal(val):\n",
    "    imgpath = \"/*.bmp\"\n",
    "#     imgpath = \"/*.png\"\n",
    "    sequ_list = []\n",
    "    exp_list = []\n",
    "    expCount = [0,0,0,0]    \n",
    "\n",
    "    for folder, exp in zip(val[0],val[1]):      \n",
    "        img_list = sorted(gb.glob(folder + imgpath))\n",
    "        img_len = len(img_list)\n",
    "        expCount[exp] += img_len\n",
    "        sequ_list.append(img_list)\n",
    "        exp_list.append(exp)\n",
    "    return sequ_list, exp_list, expCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImg(sequ_list,Gwidth,Gheight):\n",
    "    video_tensor = np.zeros((len(sequ_list), Gwidth, Gheight, 3),dtype='float')\n",
    "    for img in range(len(sequ_list)):\n",
    "#         print(sequ_list[img])\n",
    "#         print(cv2.imread(sequ_list[img]).shape)\n",
    "        frame = cv2.resize(cv2.imread(sequ_list[img]), (Gheight, Gwidth), interpolation=cv2.INTER_CUBIC)\n",
    "        video_tensor[img]=frame\n",
    "    \n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phaseBasedMagnify(sequ_list,factor,lowFreq,highFreq):\n",
    "    img_set = []\n",
    "    steer = Steerable(5)\n",
    "    pyArr = Pyramid2arr(steer)\n",
    "    fps = 100\n",
    "    Gwidth=224\n",
    "    Gheight=224\n",
    "    for sequ in sequ_list:\n",
    "        video_tensor = loadImg(sequ,Gwidth,Gheight)\n",
    "        final=np.zeros(video_tensor.shape)\n",
    "        # setup temporal filter\n",
    "        filter = IdealFilterWindowed(len(sequ), lowFreq, highFreq, fps, outfun=lambda x: x[0])\n",
    "        filter = ButterBandpassFilter(1, lowFreq, highFreq, fps)\n",
    "\n",
    "        for frameNr in range(len(sequ)):\n",
    "    #         print (frameNr)\n",
    "            sys.stdout.flush()\n",
    "    #         if frameNr < nrFrames:\n",
    "            # read frame\n",
    "            im = video_tensor[frameNr]\n",
    "            im = im.astype('uint8')\n",
    "            if im is None:\n",
    "                # if unexpected, quit\n",
    "                break\n",
    "    #           convert to gray image\n",
    "            if len(im.shape) > 2:\n",
    "                 grayIm = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                # already a grayscale image?\n",
    "                grayIm = im\n",
    "\n",
    "            # get coeffs for pyramid\n",
    "            coeff = steer.buildSCFpyr(grayIm)\n",
    "\n",
    "            # add image pyramid to video array\n",
    "            # NOTE: on first frame, this will init rotating array to store the pyramid coeffs                 \n",
    "            arr = pyArr.p2a(coeff)\n",
    "\n",
    "            phases = np.angle(arr)\n",
    "\n",
    "            # add to temporal filter\n",
    "            filter.update([phases])\n",
    "\n",
    "            # try to get filtered output to continue            \n",
    "            try:\n",
    "                filteredPhases = filter.next()\n",
    "            except StopIteration: \n",
    "                continue\n",
    "\n",
    "            #motion magnification\n",
    "            magnifiedPhases = (phases - filteredPhases) + filteredPhases*factor\n",
    "\n",
    "            # create new array\n",
    "            newArr = np.abs(arr) * np.exp(magnifiedPhases * 1j)\n",
    "\n",
    "            # create pyramid coeffs  \n",
    "            newCoeff = pyArr.a2p(newArr)\n",
    "\n",
    "            # reconstruct pyramid\n",
    "            out = steer.reconSCFpyr(newCoeff)\n",
    "\n",
    "            # clip values out of range\n",
    "            out[out>255] = 255\n",
    "            out[out<0] = 0\n",
    "\n",
    "            # make a RGB image\n",
    "            rgbIm = np.empty( (out.shape[0], out.shape[1], 3 ) )\n",
    "            rgbIm[:,:,0] = out\n",
    "            rgbIm[:,:,1] = out\n",
    "            rgbIm[:,:,2] = out\n",
    "\n",
    "            #write to disk\n",
    "            res = cv2.convertScaleAbs(rgbIm)\n",
    "    #             print('res:',res.shape)\n",
    "        final[frameNr]=res\n",
    "    img_set.append(final)   \n",
    "    return img_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e9ad724daf26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtrain_test_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputImagewithSlide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtra_tes_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseleframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#get training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain_test_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphaseBasedMagnify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlowFreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhighFreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#Phase-based magnification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# train_list, test_list, train_label, test_label = train_test_split(train_test_list, train_test_label, \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#                                       test_size=1/k, random_state=42)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-799880876527>\u001b[0m in \u001b[0;36mphaseBasedMagnify\u001b[0;34m(sequ_list, factor, lowFreq, highFreq)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# try to get filtered output to continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mfilteredPhases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/VM+VGGFace2+LSTM/temporal_filters.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mButterFilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mout_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout_low\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/VM+VGGFace2+LSTM/temporal_filters.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mwinx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSlidingWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mwiny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindowy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindowy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datapaths = \"../SMIC/SMIC_all_cropped/HS/*/\" \n",
    "k=5\n",
    "v=0\n",
    "factor = 20\n",
    "# low ideal filter\n",
    "lowFreq = 15\n",
    "# high ideal filter\n",
    "highFreq = 25\n",
    "seleframe = 22\n",
    "Val_tra_tes_list = CrossAllocation(datapaths,k)\n",
    "batch_size = 40\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # use CPU or GPU\n",
    "\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "#Set transformation\n",
    "tran_t = transforms.Compose(\n",
    "    [transforms.Resize([224,224]), \n",
    "     transforms.ToTensor(),     \n",
    "    ])\n",
    "val = Val_tra_tes_list[v] \n",
    "# valid_list, valid_label, val_expCount = InputImageforVal(val, seleframe)\n",
    "# for v in valid_list:\n",
    "#     print(len(v))\n",
    "tra_tes_list = np.delete(Val_tra_tes_list,v,axis = 0) \n",
    "train_test_list, train_test_label, expCount = InputImagewithSlide(tra_tes_list, seleframe)#get training and testing data\n",
    "\n",
    "train_test_img = phaseBasedMagnify(train_test_list,factor,lowFreq,highFreq)#Phase-based magnification\n",
    "# train_list, test_list, train_label, test_label = train_test_split(train_test_list, train_test_label, \\\n",
    "#                                       test_size=1/k, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
